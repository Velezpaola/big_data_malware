{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf5e160",
   "metadata": {},
   "source": [
    "# Regresión Logística "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a6d31",
   "metadata": {},
   "source": [
    "Utilizaremos el modelo de Regresión Logística para la clasificación de nuestros datos de prueba y analizaremos algunas métricas para obtener información de si el modelo es óptimo para la clasificación de tipo de virus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4f5751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyArrow\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /storage/apps/miniconda3/lib/python3.8/site-packages (from PyArrow) (1.21.4)\n",
      "Installing collected packages: PyArrow\n",
      "Successfully installed PyArrow-8.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324b6476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos sesion de Spark\n",
    "#!pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"LogisticRegression\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70a7b53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Lectura de datos\n",
    "dataframe = spark.read.parquet(\"malware_data_rev2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81cd7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos el formato de los datos\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "dataframe = dataframe.withColumn(\"size\", dataframe[\"size\"].cast(\"float\"))\n",
    "\n",
    "for element in dataframe.columns[3:]:     \n",
    "    dataframe = dataframe.withColumn(element, dataframe[element].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbabfdc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- : long (nullable = true)\n",
      " |-- ID: string (nullable = true)\n",
      " |-- size: float (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      " |-- 5: integer (nullable = true)\n",
      " |-- 6: integer (nullable = true)\n",
      " |-- 7: integer (nullable = true)\n",
      " |-- 8: integer (nullable = true)\n",
      " |-- 9: integer (nullable = true)\n",
      " |-- 0a: integer (nullable = true)\n",
      " |-- 0b: integer (nullable = true)\n",
      " |-- 0c: integer (nullable = true)\n",
      " |-- 0d: integer (nullable = true)\n",
      " |-- 0e: integer (nullable = true)\n",
      " |-- 0f: integer (nullable = true)\n",
      " |-- 10: integer (nullable = true)\n",
      " |-- 11: integer (nullable = true)\n",
      " |-- 12: integer (nullable = true)\n",
      " |-- 13: integer (nullable = true)\n",
      " |-- 14: integer (nullable = true)\n",
      " |-- 15: integer (nullable = true)\n",
      " |-- 16: integer (nullable = true)\n",
      " |-- 17: integer (nullable = true)\n",
      " |-- 18: integer (nullable = true)\n",
      " |-- 19: integer (nullable = true)\n",
      " |-- 1a: integer (nullable = true)\n",
      " |-- 1b: integer (nullable = true)\n",
      " |-- 1c: integer (nullable = true)\n",
      " |-- 1d: integer (nullable = true)\n",
      " |-- 1e: integer (nullable = true)\n",
      " |-- 1f: integer (nullable = true)\n",
      " |-- 20: integer (nullable = true)\n",
      " |-- 21: integer (nullable = true)\n",
      " |-- 22: integer (nullable = true)\n",
      " |-- 23: integer (nullable = true)\n",
      " |-- 24: integer (nullable = true)\n",
      " |-- 25: integer (nullable = true)\n",
      " |-- 26: integer (nullable = true)\n",
      " |-- 27: integer (nullable = true)\n",
      " |-- 28: integer (nullable = true)\n",
      " |-- 29: integer (nullable = true)\n",
      " |-- 2a: integer (nullable = true)\n",
      " |-- 2b: integer (nullable = true)\n",
      " |-- 2c: integer (nullable = true)\n",
      " |-- 2d: integer (nullable = true)\n",
      " |-- 2e: integer (nullable = true)\n",
      " |-- 2f: integer (nullable = true)\n",
      " |-- 30: integer (nullable = true)\n",
      " |-- 31: integer (nullable = true)\n",
      " |-- 32: integer (nullable = true)\n",
      " |-- 33: integer (nullable = true)\n",
      " |-- 34: integer (nullable = true)\n",
      " |-- 35: integer (nullable = true)\n",
      " |-- 36: integer (nullable = true)\n",
      " |-- 37: integer (nullable = true)\n",
      " |-- 38: integer (nullable = true)\n",
      " |-- 39: integer (nullable = true)\n",
      " |-- 3a: integer (nullable = true)\n",
      " |-- 3b: integer (nullable = true)\n",
      " |-- 3c: integer (nullable = true)\n",
      " |-- 3d: integer (nullable = true)\n",
      " |-- 3e: integer (nullable = true)\n",
      " |-- 3f: integer (nullable = true)\n",
      " |-- 40: integer (nullable = true)\n",
      " |-- 41: integer (nullable = true)\n",
      " |-- 42: integer (nullable = true)\n",
      " |-- 43: integer (nullable = true)\n",
      " |-- 44: integer (nullable = true)\n",
      " |-- 45: integer (nullable = true)\n",
      " |-- 46: integer (nullable = true)\n",
      " |-- 47: integer (nullable = true)\n",
      " |-- 48: integer (nullable = true)\n",
      " |-- 49: integer (nullable = true)\n",
      " |-- 4a: integer (nullable = true)\n",
      " |-- 4b: integer (nullable = true)\n",
      " |-- 4c: integer (nullable = true)\n",
      " |-- 4d: integer (nullable = true)\n",
      " |-- 4e: integer (nullable = true)\n",
      " |-- 4f: integer (nullable = true)\n",
      " |-- 50: integer (nullable = true)\n",
      " |-- 51: integer (nullable = true)\n",
      " |-- 52: integer (nullable = true)\n",
      " |-- 53: integer (nullable = true)\n",
      " |-- 54: integer (nullable = true)\n",
      " |-- 55: integer (nullable = true)\n",
      " |-- 56: integer (nullable = true)\n",
      " |-- 57: integer (nullable = true)\n",
      " |-- 58: integer (nullable = true)\n",
      " |-- 59: integer (nullable = true)\n",
      " |-- 5a: integer (nullable = true)\n",
      " |-- 5b: integer (nullable = true)\n",
      " |-- 5c: integer (nullable = true)\n",
      " |-- 5d: integer (nullable = true)\n",
      " |-- 5e: integer (nullable = true)\n",
      " |-- 5f: integer (nullable = true)\n",
      " |-- 60: integer (nullable = true)\n",
      " |-- 61: integer (nullable = true)\n",
      " |-- 62: integer (nullable = true)\n",
      " |-- 63: integer (nullable = true)\n",
      " |-- 64: integer (nullable = true)\n",
      " |-- 65: integer (nullable = true)\n",
      " |-- 66: integer (nullable = true)\n",
      " |-- 67: integer (nullable = true)\n",
      " |-- 68: integer (nullable = true)\n",
      " |-- 69: integer (nullable = true)\n",
      " |-- 6a: integer (nullable = true)\n",
      " |-- 6b: integer (nullable = true)\n",
      " |-- 6c: integer (nullable = true)\n",
      " |-- 6d: integer (nullable = true)\n",
      " |-- 6e: integer (nullable = true)\n",
      " |-- 6f: integer (nullable = true)\n",
      " |-- 70: integer (nullable = true)\n",
      " |-- 71: integer (nullable = true)\n",
      " |-- 72: integer (nullable = true)\n",
      " |-- 73: integer (nullable = true)\n",
      " |-- 74: integer (nullable = true)\n",
      " |-- 75: integer (nullable = true)\n",
      " |-- 76: integer (nullable = true)\n",
      " |-- 77: integer (nullable = true)\n",
      " |-- 78: integer (nullable = true)\n",
      " |-- 79: integer (nullable = true)\n",
      " |-- 7a: integer (nullable = true)\n",
      " |-- 7b: integer (nullable = true)\n",
      " |-- 7c: integer (nullable = true)\n",
      " |-- 7d: integer (nullable = true)\n",
      " |-- 7e: integer (nullable = true)\n",
      " |-- 7f: integer (nullable = true)\n",
      " |-- 80: integer (nullable = true)\n",
      " |-- 81: integer (nullable = true)\n",
      " |-- 82: integer (nullable = true)\n",
      " |-- 83: integer (nullable = true)\n",
      " |-- 84: integer (nullable = true)\n",
      " |-- 85: integer (nullable = true)\n",
      " |-- 86: integer (nullable = true)\n",
      " |-- 87: integer (nullable = true)\n",
      " |-- 88: integer (nullable = true)\n",
      " |-- 89: integer (nullable = true)\n",
      " |-- 8a: integer (nullable = true)\n",
      " |-- 8b: integer (nullable = true)\n",
      " |-- 8c: integer (nullable = true)\n",
      " |-- 8d: integer (nullable = true)\n",
      " |-- 8e: integer (nullable = true)\n",
      " |-- 8f: integer (nullable = true)\n",
      " |-- 90: integer (nullable = true)\n",
      " |-- 91: integer (nullable = true)\n",
      " |-- 92: integer (nullable = true)\n",
      " |-- 93: integer (nullable = true)\n",
      " |-- 94: integer (nullable = true)\n",
      " |-- 95: integer (nullable = true)\n",
      " |-- 96: integer (nullable = true)\n",
      " |-- 97: integer (nullable = true)\n",
      " |-- 98: integer (nullable = true)\n",
      " |-- 99: integer (nullable = true)\n",
      " |-- 9a: integer (nullable = true)\n",
      " |-- 9b: integer (nullable = true)\n",
      " |-- 9c: integer (nullable = true)\n",
      " |-- 9d: integer (nullable = true)\n",
      " |-- 9e: integer (nullable = true)\n",
      " |-- 9f: integer (nullable = true)\n",
      " |-- a0: integer (nullable = true)\n",
      " |-- a1: integer (nullable = true)\n",
      " |-- a2: integer (nullable = true)\n",
      " |-- a3: integer (nullable = true)\n",
      " |-- a4: integer (nullable = true)\n",
      " |-- a5: integer (nullable = true)\n",
      " |-- a6: integer (nullable = true)\n",
      " |-- a7: integer (nullable = true)\n",
      " |-- a8: integer (nullable = true)\n",
      " |-- a9: integer (nullable = true)\n",
      " |-- aa: integer (nullable = true)\n",
      " |-- ab: integer (nullable = true)\n",
      " |-- ac: integer (nullable = true)\n",
      " |-- ad: integer (nullable = true)\n",
      " |-- ae: integer (nullable = true)\n",
      " |-- af: integer (nullable = true)\n",
      " |-- b0: integer (nullable = true)\n",
      " |-- b1: integer (nullable = true)\n",
      " |-- b2: integer (nullable = true)\n",
      " |-- b3: integer (nullable = true)\n",
      " |-- b4: integer (nullable = true)\n",
      " |-- b5: integer (nullable = true)\n",
      " |-- b6: integer (nullable = true)\n",
      " |-- b7: integer (nullable = true)\n",
      " |-- b8: integer (nullable = true)\n",
      " |-- b9: integer (nullable = true)\n",
      " |-- ba: integer (nullable = true)\n",
      " |-- bb: integer (nullable = true)\n",
      " |-- bc: integer (nullable = true)\n",
      " |-- bd: integer (nullable = true)\n",
      " |-- be: integer (nullable = true)\n",
      " |-- bf: integer (nullable = true)\n",
      " |-- c0: integer (nullable = true)\n",
      " |-- c1: integer (nullable = true)\n",
      " |-- c2: integer (nullable = true)\n",
      " |-- c3: integer (nullable = true)\n",
      " |-- c4: integer (nullable = true)\n",
      " |-- c5: integer (nullable = true)\n",
      " |-- c6: integer (nullable = true)\n",
      " |-- c7: integer (nullable = true)\n",
      " |-- c8: integer (nullable = true)\n",
      " |-- c9: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- cb: integer (nullable = true)\n",
      " |-- cc: integer (nullable = true)\n",
      " |-- cd: integer (nullable = true)\n",
      " |-- ce: integer (nullable = true)\n",
      " |-- cf: integer (nullable = true)\n",
      " |-- d0: integer (nullable = true)\n",
      " |-- d1: integer (nullable = true)\n",
      " |-- d2: integer (nullable = true)\n",
      " |-- d3: integer (nullable = true)\n",
      " |-- d4: integer (nullable = true)\n",
      " |-- d5: integer (nullable = true)\n",
      " |-- d6: integer (nullable = true)\n",
      " |-- d7: integer (nullable = true)\n",
      " |-- d8: integer (nullable = true)\n",
      " |-- d9: integer (nullable = true)\n",
      " |-- da: integer (nullable = true)\n",
      " |-- db: integer (nullable = true)\n",
      " |-- dc: integer (nullable = true)\n",
      " |-- dd: integer (nullable = true)\n",
      " |-- de: integer (nullable = true)\n",
      " |-- df: integer (nullable = true)\n",
      " |-- e0: integer (nullable = true)\n",
      " |-- e1: integer (nullable = true)\n",
      " |-- e2: integer (nullable = true)\n",
      " |-- e3: integer (nullable = true)\n",
      " |-- e4: integer (nullable = true)\n",
      " |-- e5: integer (nullable = true)\n",
      " |-- e6: integer (nullable = true)\n",
      " |-- e7: integer (nullable = true)\n",
      " |-- e8: integer (nullable = true)\n",
      " |-- e9: integer (nullable = true)\n",
      " |-- ea: integer (nullable = true)\n",
      " |-- eb: integer (nullable = true)\n",
      " |-- ec: integer (nullable = true)\n",
      " |-- ed: integer (nullable = true)\n",
      " |-- ee: integer (nullable = true)\n",
      " |-- ef: integer (nullable = true)\n",
      " |-- f0: integer (nullable = true)\n",
      " |-- f1: integer (nullable = true)\n",
      " |-- f2: integer (nullable = true)\n",
      " |-- f3: integer (nullable = true)\n",
      " |-- f4: integer (nullable = true)\n",
      " |-- f5: integer (nullable = true)\n",
      " |-- f6: integer (nullable = true)\n",
      " |-- f7: integer (nullable = true)\n",
      " |-- f8: integer (nullable = true)\n",
      " |-- f9: integer (nullable = true)\n",
      " |-- fa: integer (nullable = true)\n",
      " |-- fb: integer (nullable = true)\n",
      " |-- fc: integer (nullable = true)\n",
      " |-- fd: integer (nullable = true)\n",
      " |-- fe: integer (nullable = true)\n",
      " |-- ff: integer (nullable = true)\n",
      " |-- ??: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6b4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "df=dataframe.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53668397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 14:47:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "22/05/10 14:47:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/storage/apps/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/pandas/conversion.py:202: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>size</th>\n",
       "      <th>Class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>0a</th>\n",
       "      <th>0b</th>\n",
       "      <th>0c</th>\n",
       "      <th>0d</th>\n",
       "      <th>0e</th>\n",
       "      <th>0f</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>1a</th>\n",
       "      <th>1b</th>\n",
       "      <th>1c</th>\n",
       "      <th>1d</th>\n",
       "      <th>1e</th>\n",
       "      <th>1f</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>2a</th>\n",
       "      <th>2b</th>\n",
       "      <th>2c</th>\n",
       "      <th>2d</th>\n",
       "      <th>2e</th>\n",
       "      <th>2f</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>3a</th>\n",
       "      <th>3b</th>\n",
       "      <th>3c</th>\n",
       "      <th>3d</th>\n",
       "      <th>3e</th>\n",
       "      <th>3f</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>4a</th>\n",
       "      <th>4b</th>\n",
       "      <th>4c</th>\n",
       "      <th>4d</th>\n",
       "      <th>4e</th>\n",
       "      <th>4f</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>5a</th>\n",
       "      <th>5b</th>\n",
       "      <th>5c</th>\n",
       "      <th>5d</th>\n",
       "      <th>5e</th>\n",
       "      <th>5f</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>6a</th>\n",
       "      <th>6b</th>\n",
       "      <th>6c</th>\n",
       "      <th>6d</th>\n",
       "      <th>6e</th>\n",
       "      <th>6f</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>7a</th>\n",
       "      <th>7b</th>\n",
       "      <th>7c</th>\n",
       "      <th>7d</th>\n",
       "      <th>7e</th>\n",
       "      <th>7f</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>8a</th>\n",
       "      <th>8b</th>\n",
       "      <th>8c</th>\n",
       "      <th>8d</th>\n",
       "      <th>8e</th>\n",
       "      <th>8f</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>9a</th>\n",
       "      <th>9b</th>\n",
       "      <th>9c</th>\n",
       "      <th>9d</th>\n",
       "      <th>9e</th>\n",
       "      <th>9f</th>\n",
       "      <th>a0</th>\n",
       "      <th>a1</th>\n",
       "      <th>a2</th>\n",
       "      <th>a3</th>\n",
       "      <th>a4</th>\n",
       "      <th>a5</th>\n",
       "      <th>a6</th>\n",
       "      <th>a7</th>\n",
       "      <th>a8</th>\n",
       "      <th>a9</th>\n",
       "      <th>aa</th>\n",
       "      <th>ab</th>\n",
       "      <th>ac</th>\n",
       "      <th>ad</th>\n",
       "      <th>ae</th>\n",
       "      <th>af</th>\n",
       "      <th>b0</th>\n",
       "      <th>b1</th>\n",
       "      <th>b2</th>\n",
       "      <th>b3</th>\n",
       "      <th>b4</th>\n",
       "      <th>b5</th>\n",
       "      <th>b6</th>\n",
       "      <th>b7</th>\n",
       "      <th>b8</th>\n",
       "      <th>b9</th>\n",
       "      <th>ba</th>\n",
       "      <th>bb</th>\n",
       "      <th>bc</th>\n",
       "      <th>bd</th>\n",
       "      <th>be</th>\n",
       "      <th>bf</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>ca</th>\n",
       "      <th>cb</th>\n",
       "      <th>cc</th>\n",
       "      <th>cd</th>\n",
       "      <th>ce</th>\n",
       "      <th>cf</th>\n",
       "      <th>d0</th>\n",
       "      <th>d1</th>\n",
       "      <th>d2</th>\n",
       "      <th>d3</th>\n",
       "      <th>d4</th>\n",
       "      <th>d5</th>\n",
       "      <th>d6</th>\n",
       "      <th>d7</th>\n",
       "      <th>d8</th>\n",
       "      <th>d9</th>\n",
       "      <th>da</th>\n",
       "      <th>db</th>\n",
       "      <th>dc</th>\n",
       "      <th>dd</th>\n",
       "      <th>de</th>\n",
       "      <th>df</th>\n",
       "      <th>e0</th>\n",
       "      <th>e1</th>\n",
       "      <th>e2</th>\n",
       "      <th>e3</th>\n",
       "      <th>e4</th>\n",
       "      <th>e5</th>\n",
       "      <th>e6</th>\n",
       "      <th>e7</th>\n",
       "      <th>e8</th>\n",
       "      <th>e9</th>\n",
       "      <th>ea</th>\n",
       "      <th>eb</th>\n",
       "      <th>ec</th>\n",
       "      <th>ed</th>\n",
       "      <th>ee</th>\n",
       "      <th>ef</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>f9</th>\n",
       "      <th>fa</th>\n",
       "      <th>fb</th>\n",
       "      <th>fc</th>\n",
       "      <th>fd</th>\n",
       "      <th>fe</th>\n",
       "      <th>ff</th>\n",
       "      <th>??</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>04BfoQRA6XEshiNuI7pF</td>\n",
       "      <td>8.941406</td>\n",
       "      <td>3</td>\n",
       "      <td>14119</td>\n",
       "      <td>8409</td>\n",
       "      <td>6122</td>\n",
       "      <td>6122</td>\n",
       "      <td>6228</td>\n",
       "      <td>6228</td>\n",
       "      <td>6349</td>\n",
       "      <td>6279</td>\n",
       "      <td>6112</td>\n",
       "      <td>6174</td>\n",
       "      <td>6371</td>\n",
       "      <td>6295</td>\n",
       "      <td>6184</td>\n",
       "      <td>6399</td>\n",
       "      <td>6347</td>\n",
       "      <td>6603</td>\n",
       "      <td>6312</td>\n",
       "      <td>6083</td>\n",
       "      <td>6396</td>\n",
       "      <td>6240</td>\n",
       "      <td>6180</td>\n",
       "      <td>6474</td>\n",
       "      <td>6125</td>\n",
       "      <td>6213</td>\n",
       "      <td>6278</td>\n",
       "      <td>6179</td>\n",
       "      <td>6187</td>\n",
       "      <td>6054</td>\n",
       "      <td>6408</td>\n",
       "      <td>6024</td>\n",
       "      <td>6176</td>\n",
       "      <td>6140</td>\n",
       "      <td>6223</td>\n",
       "      <td>6447</td>\n",
       "      <td>6023</td>\n",
       "      <td>6221</td>\n",
       "      <td>6125</td>\n",
       "      <td>6296</td>\n",
       "      <td>6100</td>\n",
       "      <td>6136</td>\n",
       "      <td>6054</td>\n",
       "      <td>6129</td>\n",
       "      <td>6125</td>\n",
       "      <td>6038</td>\n",
       "      <td>6055</td>\n",
       "      <td>6328</td>\n",
       "      <td>6302</td>\n",
       "      <td>6016</td>\n",
       "      <td>6139</td>\n",
       "      <td>6167</td>\n",
       "      <td>6328</td>\n",
       "      <td>6194</td>\n",
       "      <td>6157</td>\n",
       "      <td>6157</td>\n",
       "      <td>6222</td>\n",
       "      <td>6148</td>\n",
       "      <td>5848</td>\n",
       "      <td>6186</td>\n",
       "      <td>6166</td>\n",
       "      <td>6178</td>\n",
       "      <td>6199</td>\n",
       "      <td>6075</td>\n",
       "      <td>6301</td>\n",
       "      <td>6139</td>\n",
       "      <td>6196</td>\n",
       "      <td>6187</td>\n",
       "      <td>6396</td>\n",
       "      <td>6161</td>\n",
       "      <td>6174</td>\n",
       "      <td>6156</td>\n",
       "      <td>6202</td>\n",
       "      <td>6123</td>\n",
       "      <td>6238</td>\n",
       "      <td>6050</td>\n",
       "      <td>6270</td>\n",
       "      <td>6190</td>\n",
       "      <td>6231</td>\n",
       "      <td>6065</td>\n",
       "      <td>6438</td>\n",
       "      <td>6152</td>\n",
       "      <td>6266</td>\n",
       "      <td>6322</td>\n",
       "      <td>6235</td>\n",
       "      <td>6031</td>\n",
       "      <td>6347</td>\n",
       "      <td>6427</td>\n",
       "      <td>6128</td>\n",
       "      <td>6320</td>\n",
       "      <td>6031</td>\n",
       "      <td>6402</td>\n",
       "      <td>6410</td>\n",
       "      <td>6120</td>\n",
       "      <td>6405</td>\n",
       "      <td>6203</td>\n",
       "      <td>6087</td>\n",
       "      <td>6231</td>\n",
       "      <td>6320</td>\n",
       "      <td>6241</td>\n",
       "      <td>6123</td>\n",
       "      <td>6436</td>\n",
       "      <td>6302</td>\n",
       "      <td>6211</td>\n",
       "      <td>6172</td>\n",
       "      <td>5993</td>\n",
       "      <td>6170</td>\n",
       "      <td>6078</td>\n",
       "      <td>6397</td>\n",
       "      <td>6114</td>\n",
       "      <td>6209</td>\n",
       "      <td>6224</td>\n",
       "      <td>6129</td>\n",
       "      <td>6125</td>\n",
       "      <td>6272</td>\n",
       "      <td>6340</td>\n",
       "      <td>6367</td>\n",
       "      <td>6209</td>\n",
       "      <td>6184</td>\n",
       "      <td>6158</td>\n",
       "      <td>6322</td>\n",
       "      <td>6261</td>\n",
       "      <td>6024</td>\n",
       "      <td>6062</td>\n",
       "      <td>6161</td>\n",
       "      <td>6205</td>\n",
       "      <td>6167</td>\n",
       "      <td>6098</td>\n",
       "      <td>6171</td>\n",
       "      <td>6197</td>\n",
       "      <td>6610</td>\n",
       "      <td>6189</td>\n",
       "      <td>6228</td>\n",
       "      <td>6413</td>\n",
       "      <td>6083</td>\n",
       "      <td>6296</td>\n",
       "      <td>6256</td>\n",
       "      <td>6208</td>\n",
       "      <td>6400</td>\n",
       "      <td>6208</td>\n",
       "      <td>6268</td>\n",
       "      <td>6500</td>\n",
       "      <td>6169</td>\n",
       "      <td>6526</td>\n",
       "      <td>6045</td>\n",
       "      <td>6249</td>\n",
       "      <td>6143</td>\n",
       "      <td>6157</td>\n",
       "      <td>6218</td>\n",
       "      <td>6217</td>\n",
       "      <td>6424</td>\n",
       "      <td>6190</td>\n",
       "      <td>6358</td>\n",
       "      <td>6615</td>\n",
       "      <td>6213</td>\n",
       "      <td>6136</td>\n",
       "      <td>6234</td>\n",
       "      <td>6270</td>\n",
       "      <td>6161</td>\n",
       "      <td>6074</td>\n",
       "      <td>6000</td>\n",
       "      <td>6138</td>\n",
       "      <td>6101</td>\n",
       "      <td>6130</td>\n",
       "      <td>6103</td>\n",
       "      <td>6310</td>\n",
       "      <td>6145</td>\n",
       "      <td>6180</td>\n",
       "      <td>6447</td>\n",
       "      <td>5994</td>\n",
       "      <td>6224</td>\n",
       "      <td>6098</td>\n",
       "      <td>6180</td>\n",
       "      <td>6158</td>\n",
       "      <td>6303</td>\n",
       "      <td>6322</td>\n",
       "      <td>6133</td>\n",
       "      <td>6137</td>\n",
       "      <td>6262</td>\n",
       "      <td>6267</td>\n",
       "      <td>6177</td>\n",
       "      <td>6183</td>\n",
       "      <td>5976</td>\n",
       "      <td>6378</td>\n",
       "      <td>6252</td>\n",
       "      <td>6455</td>\n",
       "      <td>6103</td>\n",
       "      <td>6283</td>\n",
       "      <td>6413</td>\n",
       "      <td>6172</td>\n",
       "      <td>6294</td>\n",
       "      <td>6212</td>\n",
       "      <td>6134</td>\n",
       "      <td>6203</td>\n",
       "      <td>6298</td>\n",
       "      <td>6114</td>\n",
       "      <td>6125</td>\n",
       "      <td>6370</td>\n",
       "      <td>6184</td>\n",
       "      <td>6254</td>\n",
       "      <td>5991</td>\n",
       "      <td>6262</td>\n",
       "      <td>6284</td>\n",
       "      <td>6118</td>\n",
       "      <td>6114</td>\n",
       "      <td>6092</td>\n",
       "      <td>6384</td>\n",
       "      <td>6284</td>\n",
       "      <td>5858</td>\n",
       "      <td>6074</td>\n",
       "      <td>6224</td>\n",
       "      <td>6034</td>\n",
       "      <td>6172</td>\n",
       "      <td>5916</td>\n",
       "      <td>6180</td>\n",
       "      <td>6168</td>\n",
       "      <td>6394</td>\n",
       "      <td>6276</td>\n",
       "      <td>6086</td>\n",
       "      <td>6150</td>\n",
       "      <td>6224</td>\n",
       "      <td>6123</td>\n",
       "      <td>5860</td>\n",
       "      <td>6268</td>\n",
       "      <td>6072</td>\n",
       "      <td>6054</td>\n",
       "      <td>6074</td>\n",
       "      <td>6154</td>\n",
       "      <td>6096</td>\n",
       "      <td>6121</td>\n",
       "      <td>6151</td>\n",
       "      <td>6134</td>\n",
       "      <td>5968</td>\n",
       "      <td>6053</td>\n",
       "      <td>6365</td>\n",
       "      <td>6170</td>\n",
       "      <td>5990</td>\n",
       "      <td>6181</td>\n",
       "      <td>6265</td>\n",
       "      <td>6126</td>\n",
       "      <td>6028</td>\n",
       "      <td>6309</td>\n",
       "      <td>8169</td>\n",
       "      <td>8608</td>\n",
       "      <td>8311</td>\n",
       "      <td>6146</td>\n",
       "      <td>6070</td>\n",
       "      <td>6196</td>\n",
       "      <td>6337</td>\n",
       "      <td>6048</td>\n",
       "      <td>6163</td>\n",
       "      <td>6255</td>\n",
       "      <td>6169</td>\n",
       "      <td>5936</td>\n",
       "      <td>6240</td>\n",
       "      <td>6034</td>\n",
       "      <td>6228</td>\n",
       "      <td>9829</td>\n",
       "      <td>1517724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>05IXcWGxvnkto4sq17zZ</td>\n",
       "      <td>1.277344</td>\n",
       "      <td>2</td>\n",
       "      <td>15166</td>\n",
       "      <td>1049</td>\n",
       "      <td>566</td>\n",
       "      <td>642</td>\n",
       "      <td>779</td>\n",
       "      <td>377</td>\n",
       "      <td>478</td>\n",
       "      <td>304</td>\n",
       "      <td>1166</td>\n",
       "      <td>183</td>\n",
       "      <td>228</td>\n",
       "      <td>188</td>\n",
       "      <td>737</td>\n",
       "      <td>211</td>\n",
       "      <td>124</td>\n",
       "      <td>810</td>\n",
       "      <td>789</td>\n",
       "      <td>121</td>\n",
       "      <td>84</td>\n",
       "      <td>94</td>\n",
       "      <td>377</td>\n",
       "      <td>448</td>\n",
       "      <td>102</td>\n",
       "      <td>56</td>\n",
       "      <td>318</td>\n",
       "      <td>102</td>\n",
       "      <td>55</td>\n",
       "      <td>83</td>\n",
       "      <td>241</td>\n",
       "      <td>84</td>\n",
       "      <td>66</td>\n",
       "      <td>169</td>\n",
       "      <td>750</td>\n",
       "      <td>77</td>\n",
       "      <td>70</td>\n",
       "      <td>117</td>\n",
       "      <td>901</td>\n",
       "      <td>112</td>\n",
       "      <td>74</td>\n",
       "      <td>92</td>\n",
       "      <td>211</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>191</td>\n",
       "      <td>120</td>\n",
       "      <td>91</td>\n",
       "      <td>83</td>\n",
       "      <td>43</td>\n",
       "      <td>275</td>\n",
       "      <td>113</td>\n",
       "      <td>91</td>\n",
       "      <td>759</td>\n",
       "      <td>162</td>\n",
       "      <td>190</td>\n",
       "      <td>70</td>\n",
       "      <td>75</td>\n",
       "      <td>201</td>\n",
       "      <td>301</td>\n",
       "      <td>88</td>\n",
       "      <td>531</td>\n",
       "      <td>194</td>\n",
       "      <td>190</td>\n",
       "      <td>54</td>\n",
       "      <td>101</td>\n",
       "      <td>639</td>\n",
       "      <td>229</td>\n",
       "      <td>139</td>\n",
       "      <td>214</td>\n",
       "      <td>565</td>\n",
       "      <td>1497</td>\n",
       "      <td>1958</td>\n",
       "      <td>210</td>\n",
       "      <td>242</td>\n",
       "      <td>140</td>\n",
       "      <td>102</td>\n",
       "      <td>105</td>\n",
       "      <td>261</td>\n",
       "      <td>614</td>\n",
       "      <td>151</td>\n",
       "      <td>115</td>\n",
       "      <td>1231</td>\n",
       "      <td>447</td>\n",
       "      <td>137</td>\n",
       "      <td>519</td>\n",
       "      <td>212</td>\n",
       "      <td>487</td>\n",
       "      <td>620</td>\n",
       "      <td>454</td>\n",
       "      <td>324</td>\n",
       "      <td>762</td>\n",
       "      <td>93</td>\n",
       "      <td>204</td>\n",
       "      <td>117</td>\n",
       "      <td>399</td>\n",
       "      <td>300</td>\n",
       "      <td>301</td>\n",
       "      <td>189</td>\n",
       "      <td>414</td>\n",
       "      <td>99</td>\n",
       "      <td>287</td>\n",
       "      <td>479</td>\n",
       "      <td>861</td>\n",
       "      <td>570</td>\n",
       "      <td>107</td>\n",
       "      <td>495</td>\n",
       "      <td>405</td>\n",
       "      <td>735</td>\n",
       "      <td>120</td>\n",
       "      <td>470</td>\n",
       "      <td>143</td>\n",
       "      <td>398</td>\n",
       "      <td>466</td>\n",
       "      <td>326</td>\n",
       "      <td>54</td>\n",
       "      <td>579</td>\n",
       "      <td>406</td>\n",
       "      <td>1633</td>\n",
       "      <td>1600</td>\n",
       "      <td>224</td>\n",
       "      <td>180</td>\n",
       "      <td>186</td>\n",
       "      <td>171</td>\n",
       "      <td>95</td>\n",
       "      <td>79</td>\n",
       "      <td>233</td>\n",
       "      <td>439</td>\n",
       "      <td>123</td>\n",
       "      <td>162</td>\n",
       "      <td>439</td>\n",
       "      <td>281</td>\n",
       "      <td>58</td>\n",
       "      <td>1683</td>\n",
       "      <td>385</td>\n",
       "      <td>1286</td>\n",
       "      <td>75</td>\n",
       "      <td>63</td>\n",
       "      <td>331</td>\n",
       "      <td>1580</td>\n",
       "      <td>259</td>\n",
       "      <td>83020</td>\n",
       "      <td>205</td>\n",
       "      <td>1065</td>\n",
       "      <td>65</td>\n",
       "      <td>66</td>\n",
       "      <td>232</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>77</td>\n",
       "      <td>112</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>130</td>\n",
       "      <td>73</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>861</td>\n",
       "      <td>846</td>\n",
       "      <td>56</td>\n",
       "      <td>31</td>\n",
       "      <td>93</td>\n",
       "      <td>170</td>\n",
       "      <td>43</td>\n",
       "      <td>136</td>\n",
       "      <td>194</td>\n",
       "      <td>139</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>115</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>104</td>\n",
       "      <td>171</td>\n",
       "      <td>70</td>\n",
       "      <td>41</td>\n",
       "      <td>45</td>\n",
       "      <td>95</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>87</td>\n",
       "      <td>174</td>\n",
       "      <td>90</td>\n",
       "      <td>101</td>\n",
       "      <td>178</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>54</td>\n",
       "      <td>101</td>\n",
       "      <td>113</td>\n",
       "      <td>131</td>\n",
       "      <td>75</td>\n",
       "      <td>907</td>\n",
       "      <td>2712</td>\n",
       "      <td>4788</td>\n",
       "      <td>2166</td>\n",
       "      <td>396</td>\n",
       "      <td>99</td>\n",
       "      <td>345</td>\n",
       "      <td>2778</td>\n",
       "      <td>5651</td>\n",
       "      <td>278</td>\n",
       "      <td>3223</td>\n",
       "      <td>3144</td>\n",
       "      <td>363</td>\n",
       "      <td>70</td>\n",
       "      <td>2413</td>\n",
       "      <td>3151</td>\n",
       "      <td>4771</td>\n",
       "      <td>3207</td>\n",
       "      <td>139</td>\n",
       "      <td>2495</td>\n",
       "      <td>179</td>\n",
       "      <td>49</td>\n",
       "      <td>3981</td>\n",
       "      <td>5434</td>\n",
       "      <td>3271</td>\n",
       "      <td>2379</td>\n",
       "      <td>3130</td>\n",
       "      <td>125</td>\n",
       "      <td>124</td>\n",
       "      <td>54</td>\n",
       "      <td>2351</td>\n",
       "      <td>1573</td>\n",
       "      <td>1162</td>\n",
       "      <td>6220</td>\n",
       "      <td>6997</td>\n",
       "      <td>3112</td>\n",
       "      <td>254</td>\n",
       "      <td>148</td>\n",
       "      <td>5462</td>\n",
       "      <td>5434</td>\n",
       "      <td>6961</td>\n",
       "      <td>2626</td>\n",
       "      <td>8507</td>\n",
       "      <td>6671</td>\n",
       "      <td>427</td>\n",
       "      <td>40</td>\n",
       "      <td>4672</td>\n",
       "      <td>1589</td>\n",
       "      <td>1913</td>\n",
       "      <td>1633</td>\n",
       "      <td>1654</td>\n",
       "      <td>3338</td>\n",
       "      <td>233</td>\n",
       "      <td>37</td>\n",
       "      <td>272</td>\n",
       "      <td>57054</td>\n",
       "      <td>3563</td>\n",
       "      <td>2526</td>\n",
       "      <td>1004</td>\n",
       "      <td>3225</td>\n",
       "      <td>23638</td>\n",
       "      <td>38048</td>\n",
       "      <td>3477</td>\n",
       "      <td>6900</td>\n",
       "      <td>16368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID      size  Class      0     1     2     3     4     5     6     7     8     9    0a    0b    0c    0d    0e    0f    10    11    12    13    14    15    16    17    18    19    1a    1b    1c    1d    1e    1f    20    21    22    23    24    25    26    27    28    29    2a    2b    2c    2d    2e    2f    30    31    32    33    34    35    36    37    38    39    3a    3b    3c    3d    3e    3f    40    41    42    43    44    45    46    47    48    49    4a    4b    4c    4d    4e    4f    50    51    52    53    54    55    56    57    58    59    5a    5b    5c    5d    5e    5f    60    61    62    63    64    65    66    67    68    69    6a    6b    6c    6d    6e    6f    70    71    72    73    74    75    76    77    78    79    7a    7b    7c    7d    7e    7f    80    81    82    83    84    85    86    87    88    89    8a     8b    8c    8d    8e    8f    90    91    92    93    94    95    96    97    98    99    9a    9b    9c    9d    9e    9f    a0    a1    a2    a3    a4    a5    a6    a7    a8    a9    aa    ab    ac    ad    ae    af    b0    b1    b2    b3    b4    b5    b6    b7    b8    b9    ba    bb    bc    bd    be    bf    c0    c1    c2    c3    c4    c5    c6    c7    c8    c9    ca    cb    cc    cd    ce    cf    d0    d1    d2    d3    d4    d5    d6    d7    d8    d9    da    db    dc    dd    de    df    e0    e1    e2    e3    e4    e5    e6    e7    e8    e9    ea    eb    ec    ed    ee    ef    f0    f1    f2    f3    f4    f5    f6     f7    f8    f9    fa    fb     fc     fd    fe    ff       ??\n",
       "0  0  04BfoQRA6XEshiNuI7pF  8.941406      3  14119  8409  6122  6122  6228  6228  6349  6279  6112  6174  6371  6295  6184  6399  6347  6603  6312  6083  6396  6240  6180  6474  6125  6213  6278  6179  6187  6054  6408  6024  6176  6140  6223  6447  6023  6221  6125  6296  6100  6136  6054  6129  6125  6038  6055  6328  6302  6016  6139  6167  6328  6194  6157  6157  6222  6148  5848  6186  6166  6178  6199  6075  6301  6139  6196  6187  6396  6161  6174  6156  6202  6123  6238  6050  6270  6190  6231  6065  6438  6152  6266  6322  6235  6031  6347  6427  6128  6320  6031  6402  6410  6120  6405  6203  6087  6231  6320  6241  6123  6436  6302  6211  6172  5993  6170  6078  6397  6114  6209  6224  6129  6125  6272  6340  6367  6209  6184  6158  6322  6261  6024  6062  6161  6205  6167  6098  6171  6197  6610  6189  6228  6413  6083  6296  6256  6208  6400  6208  6268   6500  6169  6526  6045  6249  6143  6157  6218  6217  6424  6190  6358  6615  6213  6136  6234  6270  6161  6074  6000  6138  6101  6130  6103  6310  6145  6180  6447  5994  6224  6098  6180  6158  6303  6322  6133  6137  6262  6267  6177  6183  5976  6378  6252  6455  6103  6283  6413  6172  6294  6212  6134  6203  6298  6114  6125  6370  6184  6254  5991  6262  6284  6118  6114  6092  6384  6284  5858  6074  6224  6034  6172  5916  6180  6168  6394  6276  6086  6150  6224  6123  5860  6268  6072  6054  6074  6154  6096  6121  6151  6134  5968  6053  6365  6170  5990  6181  6265  6126  6028  6309  8169  8608  8311  6146  6070  6196  6337   6048  6163  6255  6169  5936   6240   6034  6228  9829  1517724\n",
       "1  1  05IXcWGxvnkto4sq17zZ  1.277344      2  15166  1049   566   642   779   377   478   304  1166   183   228   188   737   211   124   810   789   121    84    94   377   448   102    56   318   102    55    83   241    84    66   169   750    77    70   117   901   112    74    92   211    49    49   191   120    91    83    43   275   113    91   759   162   190    70    75   201   301    88   531   194   190    54   101   639   229   139   214   565  1497  1958   210   242   140   102   105   261   614   151   115  1231   447   137   519   212   487   620   454   324   762    93   204   117   399   300   301   189   414    99   287   479   861   570   107   495   405   735   120   470   143   398   466   326    54   579   406  1633  1600   224   180   186   171    95    79   233   439   123   162   439   281    58  1683   385  1286    75    63   331  1580   259  83020   205  1065    65    66   232    40    20    77   112    92    26    20   130    73    51    20   861   846    56    31    93   170    43   136   194   139    41    39   115    45    46   104   171    70    41    45    95    26    23    24    87   174    90   101   178    60    43    54   101   113   131    75   907  2712  4788  2166   396    99   345  2778  5651   278  3223  3144   363    70  2413  3151  4771  3207   139  2495   179    49  3981  5434  3271  2379  3130   125   124    54  2351  1573  1162  6220  6997  3112   254   148  5462  5434  6961  2626  8507  6671   427    40  4672  1589  1913  1633  1654  3338   233    37   272  57054  3563  2526  1004  3225  23638  38048  3477  6900    16368"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a57e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removemos la primera columna\n",
    "dataframe = dataframe.drop('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a246c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: string (nullable = true)\n",
      " |-- size: float (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      " |-- 0: integer (nullable = true)\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 2: integer (nullable = true)\n",
      " |-- 3: integer (nullable = true)\n",
      " |-- 4: integer (nullable = true)\n",
      " |-- 5: integer (nullable = true)\n",
      " |-- 6: integer (nullable = true)\n",
      " |-- 7: integer (nullable = true)\n",
      " |-- 8: integer (nullable = true)\n",
      " |-- 9: integer (nullable = true)\n",
      " |-- 0a: integer (nullable = true)\n",
      " |-- 0b: integer (nullable = true)\n",
      " |-- 0c: integer (nullable = true)\n",
      " |-- 0d: integer (nullable = true)\n",
      " |-- 0e: integer (nullable = true)\n",
      " |-- 0f: integer (nullable = true)\n",
      " |-- 10: integer (nullable = true)\n",
      " |-- 11: integer (nullable = true)\n",
      " |-- 12: integer (nullable = true)\n",
      " |-- 13: integer (nullable = true)\n",
      " |-- 14: integer (nullable = true)\n",
      " |-- 15: integer (nullable = true)\n",
      " |-- 16: integer (nullable = true)\n",
      " |-- 17: integer (nullable = true)\n",
      " |-- 18: integer (nullable = true)\n",
      " |-- 19: integer (nullable = true)\n",
      " |-- 1a: integer (nullable = true)\n",
      " |-- 1b: integer (nullable = true)\n",
      " |-- 1c: integer (nullable = true)\n",
      " |-- 1d: integer (nullable = true)\n",
      " |-- 1e: integer (nullable = true)\n",
      " |-- 1f: integer (nullable = true)\n",
      " |-- 20: integer (nullable = true)\n",
      " |-- 21: integer (nullable = true)\n",
      " |-- 22: integer (nullable = true)\n",
      " |-- 23: integer (nullable = true)\n",
      " |-- 24: integer (nullable = true)\n",
      " |-- 25: integer (nullable = true)\n",
      " |-- 26: integer (nullable = true)\n",
      " |-- 27: integer (nullable = true)\n",
      " |-- 28: integer (nullable = true)\n",
      " |-- 29: integer (nullable = true)\n",
      " |-- 2a: integer (nullable = true)\n",
      " |-- 2b: integer (nullable = true)\n",
      " |-- 2c: integer (nullable = true)\n",
      " |-- 2d: integer (nullable = true)\n",
      " |-- 2e: integer (nullable = true)\n",
      " |-- 2f: integer (nullable = true)\n",
      " |-- 30: integer (nullable = true)\n",
      " |-- 31: integer (nullable = true)\n",
      " |-- 32: integer (nullable = true)\n",
      " |-- 33: integer (nullable = true)\n",
      " |-- 34: integer (nullable = true)\n",
      " |-- 35: integer (nullable = true)\n",
      " |-- 36: integer (nullable = true)\n",
      " |-- 37: integer (nullable = true)\n",
      " |-- 38: integer (nullable = true)\n",
      " |-- 39: integer (nullable = true)\n",
      " |-- 3a: integer (nullable = true)\n",
      " |-- 3b: integer (nullable = true)\n",
      " |-- 3c: integer (nullable = true)\n",
      " |-- 3d: integer (nullable = true)\n",
      " |-- 3e: integer (nullable = true)\n",
      " |-- 3f: integer (nullable = true)\n",
      " |-- 40: integer (nullable = true)\n",
      " |-- 41: integer (nullable = true)\n",
      " |-- 42: integer (nullable = true)\n",
      " |-- 43: integer (nullable = true)\n",
      " |-- 44: integer (nullable = true)\n",
      " |-- 45: integer (nullable = true)\n",
      " |-- 46: integer (nullable = true)\n",
      " |-- 47: integer (nullable = true)\n",
      " |-- 48: integer (nullable = true)\n",
      " |-- 49: integer (nullable = true)\n",
      " |-- 4a: integer (nullable = true)\n",
      " |-- 4b: integer (nullable = true)\n",
      " |-- 4c: integer (nullable = true)\n",
      " |-- 4d: integer (nullable = true)\n",
      " |-- 4e: integer (nullable = true)\n",
      " |-- 4f: integer (nullable = true)\n",
      " |-- 50: integer (nullable = true)\n",
      " |-- 51: integer (nullable = true)\n",
      " |-- 52: integer (nullable = true)\n",
      " |-- 53: integer (nullable = true)\n",
      " |-- 54: integer (nullable = true)\n",
      " |-- 55: integer (nullable = true)\n",
      " |-- 56: integer (nullable = true)\n",
      " |-- 57: integer (nullable = true)\n",
      " |-- 58: integer (nullable = true)\n",
      " |-- 59: integer (nullable = true)\n",
      " |-- 5a: integer (nullable = true)\n",
      " |-- 5b: integer (nullable = true)\n",
      " |-- 5c: integer (nullable = true)\n",
      " |-- 5d: integer (nullable = true)\n",
      " |-- 5e: integer (nullable = true)\n",
      " |-- 5f: integer (nullable = true)\n",
      " |-- 60: integer (nullable = true)\n",
      " |-- 61: integer (nullable = true)\n",
      " |-- 62: integer (nullable = true)\n",
      " |-- 63: integer (nullable = true)\n",
      " |-- 64: integer (nullable = true)\n",
      " |-- 65: integer (nullable = true)\n",
      " |-- 66: integer (nullable = true)\n",
      " |-- 67: integer (nullable = true)\n",
      " |-- 68: integer (nullable = true)\n",
      " |-- 69: integer (nullable = true)\n",
      " |-- 6a: integer (nullable = true)\n",
      " |-- 6b: integer (nullable = true)\n",
      " |-- 6c: integer (nullable = true)\n",
      " |-- 6d: integer (nullable = true)\n",
      " |-- 6e: integer (nullable = true)\n",
      " |-- 6f: integer (nullable = true)\n",
      " |-- 70: integer (nullable = true)\n",
      " |-- 71: integer (nullable = true)\n",
      " |-- 72: integer (nullable = true)\n",
      " |-- 73: integer (nullable = true)\n",
      " |-- 74: integer (nullable = true)\n",
      " |-- 75: integer (nullable = true)\n",
      " |-- 76: integer (nullable = true)\n",
      " |-- 77: integer (nullable = true)\n",
      " |-- 78: integer (nullable = true)\n",
      " |-- 79: integer (nullable = true)\n",
      " |-- 7a: integer (nullable = true)\n",
      " |-- 7b: integer (nullable = true)\n",
      " |-- 7c: integer (nullable = true)\n",
      " |-- 7d: integer (nullable = true)\n",
      " |-- 7e: integer (nullable = true)\n",
      " |-- 7f: integer (nullable = true)\n",
      " |-- 80: integer (nullable = true)\n",
      " |-- 81: integer (nullable = true)\n",
      " |-- 82: integer (nullable = true)\n",
      " |-- 83: integer (nullable = true)\n",
      " |-- 84: integer (nullable = true)\n",
      " |-- 85: integer (nullable = true)\n",
      " |-- 86: integer (nullable = true)\n",
      " |-- 87: integer (nullable = true)\n",
      " |-- 88: integer (nullable = true)\n",
      " |-- 89: integer (nullable = true)\n",
      " |-- 8a: integer (nullable = true)\n",
      " |-- 8b: integer (nullable = true)\n",
      " |-- 8c: integer (nullable = true)\n",
      " |-- 8d: integer (nullable = true)\n",
      " |-- 8e: integer (nullable = true)\n",
      " |-- 8f: integer (nullable = true)\n",
      " |-- 90: integer (nullable = true)\n",
      " |-- 91: integer (nullable = true)\n",
      " |-- 92: integer (nullable = true)\n",
      " |-- 93: integer (nullable = true)\n",
      " |-- 94: integer (nullable = true)\n",
      " |-- 95: integer (nullable = true)\n",
      " |-- 96: integer (nullable = true)\n",
      " |-- 97: integer (nullable = true)\n",
      " |-- 98: integer (nullable = true)\n",
      " |-- 99: integer (nullable = true)\n",
      " |-- 9a: integer (nullable = true)\n",
      " |-- 9b: integer (nullable = true)\n",
      " |-- 9c: integer (nullable = true)\n",
      " |-- 9d: integer (nullable = true)\n",
      " |-- 9e: integer (nullable = true)\n",
      " |-- 9f: integer (nullable = true)\n",
      " |-- a0: integer (nullable = true)\n",
      " |-- a1: integer (nullable = true)\n",
      " |-- a2: integer (nullable = true)\n",
      " |-- a3: integer (nullable = true)\n",
      " |-- a4: integer (nullable = true)\n",
      " |-- a5: integer (nullable = true)\n",
      " |-- a6: integer (nullable = true)\n",
      " |-- a7: integer (nullable = true)\n",
      " |-- a8: integer (nullable = true)\n",
      " |-- a9: integer (nullable = true)\n",
      " |-- aa: integer (nullable = true)\n",
      " |-- ab: integer (nullable = true)\n",
      " |-- ac: integer (nullable = true)\n",
      " |-- ad: integer (nullable = true)\n",
      " |-- ae: integer (nullable = true)\n",
      " |-- af: integer (nullable = true)\n",
      " |-- b0: integer (nullable = true)\n",
      " |-- b1: integer (nullable = true)\n",
      " |-- b2: integer (nullable = true)\n",
      " |-- b3: integer (nullable = true)\n",
      " |-- b4: integer (nullable = true)\n",
      " |-- b5: integer (nullable = true)\n",
      " |-- b6: integer (nullable = true)\n",
      " |-- b7: integer (nullable = true)\n",
      " |-- b8: integer (nullable = true)\n",
      " |-- b9: integer (nullable = true)\n",
      " |-- ba: integer (nullable = true)\n",
      " |-- bb: integer (nullable = true)\n",
      " |-- bc: integer (nullable = true)\n",
      " |-- bd: integer (nullable = true)\n",
      " |-- be: integer (nullable = true)\n",
      " |-- bf: integer (nullable = true)\n",
      " |-- c0: integer (nullable = true)\n",
      " |-- c1: integer (nullable = true)\n",
      " |-- c2: integer (nullable = true)\n",
      " |-- c3: integer (nullable = true)\n",
      " |-- c4: integer (nullable = true)\n",
      " |-- c5: integer (nullable = true)\n",
      " |-- c6: integer (nullable = true)\n",
      " |-- c7: integer (nullable = true)\n",
      " |-- c8: integer (nullable = true)\n",
      " |-- c9: integer (nullable = true)\n",
      " |-- ca: integer (nullable = true)\n",
      " |-- cb: integer (nullable = true)\n",
      " |-- cc: integer (nullable = true)\n",
      " |-- cd: integer (nullable = true)\n",
      " |-- ce: integer (nullable = true)\n",
      " |-- cf: integer (nullable = true)\n",
      " |-- d0: integer (nullable = true)\n",
      " |-- d1: integer (nullable = true)\n",
      " |-- d2: integer (nullable = true)\n",
      " |-- d3: integer (nullable = true)\n",
      " |-- d4: integer (nullable = true)\n",
      " |-- d5: integer (nullable = true)\n",
      " |-- d6: integer (nullable = true)\n",
      " |-- d7: integer (nullable = true)\n",
      " |-- d8: integer (nullable = true)\n",
      " |-- d9: integer (nullable = true)\n",
      " |-- da: integer (nullable = true)\n",
      " |-- db: integer (nullable = true)\n",
      " |-- dc: integer (nullable = true)\n",
      " |-- dd: integer (nullable = true)\n",
      " |-- de: integer (nullable = true)\n",
      " |-- df: integer (nullable = true)\n",
      " |-- e0: integer (nullable = true)\n",
      " |-- e1: integer (nullable = true)\n",
      " |-- e2: integer (nullable = true)\n",
      " |-- e3: integer (nullable = true)\n",
      " |-- e4: integer (nullable = true)\n",
      " |-- e5: integer (nullable = true)\n",
      " |-- e6: integer (nullable = true)\n",
      " |-- e7: integer (nullable = true)\n",
      " |-- e8: integer (nullable = true)\n",
      " |-- e9: integer (nullable = true)\n",
      " |-- ea: integer (nullable = true)\n",
      " |-- eb: integer (nullable = true)\n",
      " |-- ec: integer (nullable = true)\n",
      " |-- ed: integer (nullable = true)\n",
      " |-- ee: integer (nullable = true)\n",
      " |-- ef: integer (nullable = true)\n",
      " |-- f0: integer (nullable = true)\n",
      " |-- f1: integer (nullable = true)\n",
      " |-- f2: integer (nullable = true)\n",
      " |-- f3: integer (nullable = true)\n",
      " |-- f4: integer (nullable = true)\n",
      " |-- f5: integer (nullable = true)\n",
      " |-- f6: integer (nullable = true)\n",
      " |-- f7: integer (nullable = true)\n",
      " |-- f8: integer (nullable = true)\n",
      " |-- f9: integer (nullable = true)\n",
      " |-- fa: integer (nullable = true)\n",
      " |-- fb: integer (nullable = true)\n",
      " |-- fc: integer (nullable = true)\n",
      " |-- fd: integer (nullable = true)\n",
      " |-- fe: integer (nullable = true)\n",
      " |-- ff: integer (nullable = true)\n",
      " |-- ??: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d431ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/10 18:18:08 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+-----+----+----+-------+--------------------+\n",
      "|                  ID|     size|Class|    0|   1|   2|   3|   4|   5|   6|   7|   8|   9|  0a|  0b|  0c|  0d|  0e|  0f|  10|  11|  12|  13|  14|  15|  16|  17|  18|  19|  1a|  1b|  1c|  1d|  1e|  1f|  20|  21|  22|  23|  24|  25|  26|  27|  28|  29|  2a|  2b|  2c|  2d|  2e|  2f|  30|  31|  32|  33|  34|  35|  36|  37|  38|  39|  3a|  3b|  3c|  3d|  3e|  3f|  40|  41|  42|  43|  44|  45|  46|  47|  48|  49|  4a|  4b|  4c|  4d|  4e|  4f|  50|  51|  52|  53|  54|  55|  56|  57|  58|  59|  5a|  5b|  5c|  5d|  5e|  5f|  60|  61|  62|  63|  64|  65|  66|  67|  68|  69|  6a|  6b|  6c|  6d|  6e|  6f|  70|  71|  72|  73|  74|  75|  76|  77|  78|  79|  7a|  7b|  7c|  7d|  7e|  7f|  80|  81|  82|  83|  84|  85|  86|  87|  88|  89|  8a|   8b|  8c|  8d|  8e|  8f|  90|  91|  92|  93|  94|  95|  96|  97|  98|  99|  9a|  9b|  9c|  9d|  9e|  9f|  a0|  a1|  a2|  a3|  a4|  a5|  a6|  a7|  a8|  a9|  aa|  ab|  ac|  ad|  ae|  af|  b0|  b1|  b2|  b3|  b4|  b5|  b6|  b7|  b8|  b9|  ba|  bb|  bc|  bd|  be|  bf|  c0|  c1|  c2|  c3|  c4|  c5|  c6|  c7|  c8|  c9|  ca|  cb|  cc|  cd|  ce|  cf|  d0|  d1|  d2|  d3|  d4|  d5|  d6|  d7|  d8|  d9|  da|  db|  dc|  dd|  de|  df|  e0|  e1|  e2|  e3|  e4|  e5|  e6|  e7|  e8|  e9|  ea|  eb|  ec|  ed|  ee|  ef|  f0|  f1|  f2|  f3|  f4|  f5|  f6|   f7|  f8|  f9|  fa|  fb|   fc|   fd|  fe|  ff|     ??|            features|\n",
      "+--------------------+---------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+-----+----+----+-------+--------------------+\n",
      "|04BfoQRA6XEshiNuI7pF| 8.941406|    3|14119|8409|6122|6122|6228|6228|6349|6279|6112|6174|6371|6295|6184|6399|6347|6603|6312|6083|6396|6240|6180|6474|6125|6213|6278|6179|6187|6054|6408|6024|6176|6140|6223|6447|6023|6221|6125|6296|6100|6136|6054|6129|6125|6038|6055|6328|6302|6016|6139|6167|6328|6194|6157|6157|6222|6148|5848|6186|6166|6178|6199|6075|6301|6139|6196|6187|6396|6161|6174|6156|6202|6123|6238|6050|6270|6190|6231|6065|6438|6152|6266|6322|6235|6031|6347|6427|6128|6320|6031|6402|6410|6120|6405|6203|6087|6231|6320|6241|6123|6436|6302|6211|6172|5993|6170|6078|6397|6114|6209|6224|6129|6125|6272|6340|6367|6209|6184|6158|6322|6261|6024|6062|6161|6205|6167|6098|6171|6197|6610|6189|6228|6413|6083|6296|6256|6208|6400|6208|6268| 6500|6169|6526|6045|6249|6143|6157|6218|6217|6424|6190|6358|6615|6213|6136|6234|6270|6161|6074|6000|6138|6101|6130|6103|6310|6145|6180|6447|5994|6224|6098|6180|6158|6303|6322|6133|6137|6262|6267|6177|6183|5976|6378|6252|6455|6103|6283|6413|6172|6294|6212|6134|6203|6298|6114|6125|6370|6184|6254|5991|6262|6284|6118|6114|6092|6384|6284|5858|6074|6224|6034|6172|5916|6180|6168|6394|6276|6086|6150|6224|6123|5860|6268|6072|6054|6074|6154|6096|6121|6151|6134|5968|6053|6365|6170|5990|6181|6265|6126|6028|6309|8169|8608|8311|6146|6070|6196|6337| 6048|6163|6255|6169|5936| 6240| 6034|6228|9829|1517724|[14119.0,8409.0,6...|\n",
      "|05IXcWGxvnkto4sq17zZ|1.2773438|    2|15166|1049| 566| 642| 779| 377| 478| 304|1166| 183| 228| 188| 737| 211| 124| 810| 789| 121|  84|  94| 377| 448| 102|  56| 318| 102|  55|  83| 241|  84|  66| 169| 750|  77|  70| 117| 901| 112|  74|  92| 211|  49|  49| 191| 120|  91|  83|  43| 275| 113|  91| 759| 162| 190|  70|  75| 201| 301|  88| 531| 194| 190|  54| 101| 639| 229| 139| 214| 565|1497|1958| 210| 242| 140| 102| 105| 261| 614| 151| 115|1231| 447| 137| 519| 212| 487| 620| 454| 324| 762|  93| 204| 117| 399| 300| 301| 189| 414|  99| 287| 479| 861| 570| 107| 495| 405| 735| 120| 470| 143| 398| 466| 326|  54| 579| 406|1633|1600| 224| 180| 186| 171|  95|  79| 233| 439| 123| 162| 439| 281|  58|1683| 385|1286|  75|  63| 331|1580| 259|83020| 205|1065|  65|  66| 232|  40|  20|  77| 112|  92|  26|  20| 130|  73|  51|  20| 861| 846|  56|  31|  93| 170|  43| 136| 194| 139|  41|  39| 115|  45|  46| 104| 171|  70|  41|  45|  95|  26|  23|  24|  87| 174|  90| 101| 178|  60|  43|  54| 101| 113| 131|  75| 907|2712|4788|2166| 396|  99| 345|2778|5651| 278|3223|3144| 363|  70|2413|3151|4771|3207| 139|2495| 179|  49|3981|5434|3271|2379|3130| 125| 124|  54|2351|1573|1162|6220|6997|3112| 254| 148|5462|5434|6961|2626|8507|6671| 427|  40|4672|1589|1913|1633|1654|3338| 233|  37| 272|57054|3563|2526|1004|3225|23638|38048|3477|6900|  16368|[15166.0,1049.0,5...|\n",
      "+--------------------+---------+-----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-----+----+----+----+----+-----+-----+----+----+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generamos una nueva columna que tenga todas las caracteristicas que usaremos(asi lo piden los modelos de pyspark)\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = list(dataframe.columns[3:])\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=numericCols, outputCol=\"features\")\n",
    "dataframe_rf = assembler.transform(dataframe)\n",
    "dataframe_rf.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6844fcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Count: 3214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Train/test split\n",
    "train, test = dataframe_rf.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4fcdc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1| 1069|\n",
      "|    6|  540|\n",
      "|    3| 2075|\n",
      "|    5|   29|\n",
      "|    9|  718|\n",
      "|    4|  344|\n",
      "|    8|  864|\n",
      "|    7|  262|\n",
      "|    2| 1753|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Observemos la distribución de las clases en los datos de prueba\n",
    "train.groupby('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99c85f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Class|count|\n",
      "+-----+-----+\n",
      "|    1| 1541|\n",
      "|    6|  751|\n",
      "|    3| 2942|\n",
      "|    5|   42|\n",
      "|    9| 1013|\n",
      "|    4|  475|\n",
      "|    8| 1228|\n",
      "|    7|  398|\n",
      "|    2| 2478|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataframe.groupby('Class').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "965a3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8714:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.9172370877411326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Se instalan las librerías necesarias para la implementación del modelo de Regresión Logística\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "#Regresión logística con parámetros default\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'Class')\n",
    "lrModel = lr.fit(train)\n",
    "prediction = lrModel.transform(test)\n",
    "accuracy = prediction.filter(prediction.Class == prediction.prediction).count() / float(prediction.count())\n",
    "\n",
    "print(\"Accuracy : \",accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ab988c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8717:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Class|prediction|\n",
      "+-----+----------+\n",
      "|    2|       2.0|\n",
      "|    2|       2.0|\n",
      "|    6|       6.0|\n",
      "|    3|       3.0|\n",
      "|    8|       8.0|\n",
      "|    1|       1.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Algunas de las predicciones\n",
    "prediction.select(\"Class\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a3de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utilizamos ParamGrid para establecer diferentes valores para los parámetros y un Evaluator para usar Cross Validation\n",
    "\n",
    "lrparamGrid = (ParamGridBuilder()\n",
    "               .addGrid(lr.regParam, [0.01, 0.1, 0.5])\n",
    "               .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "               .addGrid(lr.maxIter, [10, 50,100])\n",
    "               .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"Class\", predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25efce35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizamos Cross Validation para el modelo de Regresión Logística\n",
    "lrcv = CrossValidator(estimator = lr,\n",
    "                      estimatorParamMaps = lrparamGrid,\n",
    "                      evaluator = evaluator,\n",
    "                      numFolds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5e6c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Entrenamos el modelo con los datos de entrenamiento\n",
    "lrcvModel = lrcv.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b98ac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam):  0.01\n",
      "Best Param (MaxIter):  50\n",
      "Best Param (elasticNetParam):  0.0\n"
     ]
    }
   ],
   "source": [
    "#Calculamos los parámetros que nos resultan en el mejor modelo \n",
    "best_lr= lrcvModel.bestModel\n",
    "\n",
    "print('Best Param (regParam): ',best_lr._java_obj.getRegParam())\n",
    "print('Best Param (MaxIter): ',best_lr._java_obj.getMaxIter())\n",
    "print('Best Param (elasticNetParam): ',best_lr._java_obj.getElasticNetParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67ddd8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8603:============================>                           (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.7767209222217072\n",
      "Test Error = 0.2232790777782928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Predecimos con datos de test y calculamos el accuracy\n",
    "predictions = lrcvModel.transform(test)\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %s\" % (accuracy))\n",
    "print(\"Test Error = %s\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57fbb8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8605:>                                                       (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|Class|prediction|\n",
      "+-----+----------+\n",
      "|    2|       2.0|\n",
      "|    2|       2.0|\n",
      "|    6|       6.0|\n",
      "|    3|       3.0|\n",
      "|    8|       8.0|\n",
      "|    1|       8.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "|    2|       2.0|\n",
      "|    3|       3.0|\n",
      "+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Algunas de las predicciones\n",
    "predictions.select(\"Class\", \"prediction\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
