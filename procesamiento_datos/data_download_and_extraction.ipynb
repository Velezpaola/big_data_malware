{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba76a55e",
   "metadata": {},
   "source": [
    "### Utilizando Spark para la clasificación de Malware"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30803173",
   "metadata": {},
   "source": [
    "#### Contexto del problema\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94227455",
   "metadata": {},
   "source": [
    "El malware (o software malicioso) es un software diseñado para dañar a los usuarios, las organizaciones y los sistemas informáticos y de telecomunicaciones. Más concretamente, el malware puede bloquear la conexión a Internet, corromper un sistema operativo, robar la contraseña y otra información privada de un usuario. 1\n",
    "En este problema utilizaremos el dataset de Microsoft Malware Classification Challenge (BIG 2015), en conjunto con tecnologías tales como Spark y Hadoop, con el fin de realizar la clasificación de Malware en un contexto de Big Data. El dataset de Clasificación de Malware de Microsoft se anunció en 2015 junto con la publicación de un enorme conjunto de datos de casi 0,5 terabytes, compuesto por el desensamblaje y el código de bytes de más de 20 000 muestras de malware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcf7869",
   "metadata": {},
   "source": [
    "#### Descarga y extracción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67d47168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyunpack in /LUSTRE/home/cavv/.local/lib/python3.8/site-packages (0.2.2)\n",
      "Requirement already satisfied: entrypoint2 in /LUSTRE/home/cavv/.local/lib/python3.8/site-packages (from pyunpack) (1.0)\n",
      "Requirement already satisfied: easyprocess in /LUSTRE/home/cavv/.local/lib/python3.8/site-packages (from pyunpack) (1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting patool\n",
      "  Downloading patool-1.12-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 892 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: patool\n",
      "Successfully installed patool-1.12\n",
      "/LUSTRE/home/cavv/big data\n"
     ]
    }
   ],
   "source": [
    "#Instalando librerías necesarias\n",
    "!pip install --user kaggle\n",
    "!pip install pyunpack\n",
    "!pip install patool\n",
    "!pip install py7zr\n",
    "!python -m pip install pandas\n",
    "!python -m pip install pyarrow\n",
    "!python -m pip install pyspark-pandas\n",
    "!pip install pyspark \n",
    "\n",
    "\n",
    "#Importando librerías necesarias\n",
    "from pyunpack import Archive\n",
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyspark.pandas as ps\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f2c3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading malware-classification.zip to /LUSTRE/home/cavv/big data\n",
      "100%|█████████████████████████████████████▉| 35.3G/35.3G [09:04<00:00, 71.7MB/s]\n",
      "100%|██████████████████████████████████████| 35.3G/35.3G [09:04<00:00, 69.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Descargando el Dataset comprimido\n",
    "!kaggle competitions download -c malware-classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a19d770",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrayendo el Dataset\n",
    "path_to_zip_file = '/LUSTRE/home/cavv/big data/malware-classification.zip'\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('/LUSTRE/home/cavv/big data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "598fb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrayendo los samples\n",
    "Archive('dataSample.7z').extractall(\"/LUSTRE/home/cavv/big data/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34677526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrayendo el conjunto de entrenamiento\n",
    "Archive('train.7z').extractall(\"/LUSTRE/home/cavv/big data/training_set/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96acf0c8",
   "metadata": {},
   "source": [
    "### Extracción de caracteristicas y generación de CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb03f8",
   "metadata": {},
   "source": [
    "Pendiente editar esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8825a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "### El código fue tomado de: https://www.kaggle.com/code/paulrohan2020/microsoft-malware-detection-log-loss-of-0-0070\n",
    "### CreditOS dirigidos a Rohan-Paul-Al\n",
    "\n",
    "#removal of addres from byte files\n",
    "# contents of .byte files\n",
    "# ----------------\n",
    "#00401000 56 8D 44 24 08 50 8B F1 E8 1C 1B 00 00 C7 06 08 \n",
    "#-------------------\n",
    "#we remove the starting address 00401000\n",
    "import os\n",
    "import numpy as np\n",
    "files = os.listdir('byteFiles')\n",
    "filenames=[]\n",
    "array=[]\n",
    "for file in files:\n",
    "    if(file.endswith(\"bytes\")):\n",
    "        file=file.split('.')[0]\n",
    "        text_file = open('byteFiles/'+file+\".txt\", 'w+')\n",
    "        with open('byteFiles/'+file+\".bytes\",\"r\") as fp:\n",
    "            lines=\"\"\n",
    "            for line in fp:\n",
    "                a=line.rstrip().split(\" \")[1:]\n",
    "                b=' '.join(a)\n",
    "                b=b+\"\\n\"\n",
    "                text_file.write(b)\n",
    "            fp.close()\n",
    "            os.remove('byteFiles/'+file+\".bytes\")\n",
    "        text_file.close()\n",
    "\n",
    "files = os.listdir('byteFiles')\n",
    "filenames2=[]\n",
    "feature_matrix = np.zeros((len(files),257),dtype=int)\n",
    "k=0\n",
    "\n",
    "\n",
    "\n",
    "# program to convert into bag of words of bytefiles\n",
    "# this is custom-built bag of words this is unigram bag of words\n",
    "# This is a Custom Implementation of CountVectorizer as CountVectorizer will NOT suport working on such huge file system of 50GB\n",
    "# For this Uni-Gram feature creating and writing to a file named 'result.csv'\n",
    "\n",
    "byte_feature_file=open('result.csv','w+')\n",
    "byte_feature_file.write(\"ID,0,1,2,3,4,5,6,7,8,9,0a,0b,0c,0d,0e,0f,10,11,12,13,14,15,16,17,18,19,1a,1b,1c,1d,1e,1f,20,21,22,23,24,25,26,27,28,29,2a,2b,2c,2d,2e,2f,30,31,32,33,34,35,36,37,38,39,3a,3b,3c,3d,3e,3f,40,41,42,43,44,45,46,47,48,49,4a,4b,4c,4d,4e,4f,50,51,52,53,54,55,56,57,58,59,5a,5b,5c,5d,5e,5f,60,61,62,63,64,65,66,67,68,69,6a,6b,6c,6d,6e,6f,70,71,72,73,74,75,76,77,78,79,7a,7b,7c,7d,7e,7f,80,81,82,83,84,85,86,87,88,89,8a,8b,8c,8d,8e,8f,90,91,92,93,94,95,96,97,98,99,9a,9b,9c,9d,9e,9f,a0,a1,a2,a3,a4,a5,a6,a7,a8,a9,aa,ab,ac,ad,ae,af,b0,b1,b2,b3,b4,b5,b6,b7,b8,b9,ba,bb,bc,bd,be,bf,c0,c1,c2,c3,c4,c5,c6,c7,c8,c9,ca,cb,cc,cd,ce,cf,d0,d1,d2,d3,d4,d5,d6,d7,d8,d9,da,db,dc,dd,de,df,e0,e1,e2,e3,e4,e5,e6,e7,e8,e9,ea,eb,ec,ed,ee,ef,f0,f1,f2,f3,f4,f5,f6,f7,f8,f9,fa,fb,fc,fd,fe,ff,??\")\n",
    "\n",
    "byte_feature_file.write(\"\\n\")\n",
    "\n",
    "for file in files:\n",
    "    filenames2.append(file)\n",
    "    byte_feature_file.write(file+\",\")\n",
    "    if(file.endswith(\"txt\")):\n",
    "        with open('byteFiles/'+file,\"r\") as byte_flie:\n",
    "            for lines in byte_flie:\n",
    "                line=lines.rstrip().split(\" \")\n",
    "                for hex_code in line:\n",
    "                    if hex_code=='??':\n",
    "                        feature_matrix[k][256]+=1\n",
    "                    else:\n",
    "                        feature_matrix[k][int(hex_code,16)]+=1\n",
    "        byte_flie.close()\n",
    "    for i, row in enumerate(feature_matrix[k]):\n",
    "        if i!=len(feature_matrix[k])-1:\n",
    "            byte_feature_file.write(str(row)+\",\")\n",
    "        else:\n",
    "            byte_feature_file.write(str(row))\n",
    "    byte_feature_file.write(\"\\n\")\n",
    "    \n",
    "    k += 1\n",
    "\n",
    "byte_feature_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b14adb9",
   "metadata": {},
   "source": [
    "#### Uso de peso de archivo como caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0d6aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading labels using pyspark-pandas\n",
    "label_data = ps.read_csv(\"trainLabels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd5aac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500ac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing first 5 rows\n",
    "label_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4108ef68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction: Using file size of bytes files as features\n",
    "# file sizes of byte files\n",
    "# Code modified from https://www.kaggle.com/code/paulrohan2020/microsoft-malware-detection-log-loss-of-0-0070\n",
    "# all credits go to Rohan-Paul-Al\n",
    "\n",
    "files=os.listdir('training_set/byteFiles')\n",
    "\n",
    "filenames=label_data['Id'].tolist()\n",
    "class_y=label_data['Class'].tolist()\n",
    "class_bytes=[]\n",
    "sizebytes=[]\n",
    "fnames=[]\n",
    "for file in files:\n",
    "    print('Trying for ', file)\n",
    "    statinfo=os.stat('training_set/byteFiles/'+file)\n",
    "    # split the file name at '.' and take the first part of it i.e the file name\n",
    "    file=file.split('.')[0]\n",
    "    print(file)\n",
    "    if any(file == filename for filename in filenames):\n",
    "        i=filenames.index(file)\n",
    "        class_bytes.append(class_y[i])\n",
    "        # converting into Mb's\n",
    "        sizebytes.append(statinfo.st_size/(1024.0*1024.0))\n",
    "        fnames.append(file)\n",
    "\n",
    "data_size_byte=ps.DataFrame({'ID':fnames,'size':sizebytes,'Class':class_bytes})\n",
    "print (data_size_byte.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ce828",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking first 5 rows of pyspark-pandas dataframe\n",
    "data_size_byte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d284f730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting to csv\n",
    "data_size_byte.to_spark().toPandas().to_csv('file_size_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475d44d",
   "metadata": {},
   "source": [
    "#### Uniendo Dataframes de caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"data merging\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7efd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76db57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size_data = spark.read.csv('file_size_data.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef21428",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size_data = file_size_data.drop('_c0')\n",
    "file_size_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301b353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_data = spark.read.csv('training_set/result.csv',  header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d5964",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad5028",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing Unigram Data\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "\n",
    "unigram_data = unigram_data.withColumn('ID', regexp_replace(\"ID\", \".txt\", \"\"))\n",
    "unigram_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JOining both datasets\n",
    "malware_data = file_size_data.join(unigram_data, 'ID', how='left')\n",
    "malware_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c34bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporting as csv\n",
    "malware_data.toPandas().to_csv('malware_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f11241",
   "metadata": {},
   "source": [
    "\n",
    "#### Transformando CSV a parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baeebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "import os # libreria de manejo del sistema operativo\n",
    "os.system(\"wget -q https://www-us.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\")\n",
    "os.system(\"tar xf /spark-2.4.5-bin-hadoop2.7.tgz\")\n",
    "# instalar pyspark\n",
    "!pip install -q pyspark\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d728dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = spark.read.csv('malware_data.csv', header=True, sep =',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb284fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d4cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe=data_csv.drop('_c0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3d4e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cambiamos el formato de los datos\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "dataframe = dataframe.withColumn(\"size\", dataframe[\"size\"].cast(\"float\"))\n",
    "\n",
    "for element in df.columns[2:]:     \n",
    "    dataframe = dataframe.withColumn(element, dataframe[element].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7521e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3cc130",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.write.mode('overwrite').parquet('malware.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eede12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"malware.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f64a8b",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9c302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removemos la columna ID pues no es necesaria para realizar el split\n",
    "df = df.drop('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04440733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITS\n",
    "split1, test = df.randomSplit(weights=[0.8,0.2], seed=17)\n",
    "train, validation = split1.randomSplit(weights=[0.7,0.3], seed=17)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
